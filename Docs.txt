Spark is an open source software developed by UC Berkeley RAD lab in 2009 and later donated to Apache Software Foundation(February 2014).
Spark is a big data solution, proven to be easier and faster than Hadoop MapReduce and can run applications parallelly on the distributed cluster (multiple nodes).
Spark is written in Scala and later on due to its industry adaptation it’s API PySpark released for Python using Py4J. 


Apache Spark Features:-
time processing framework
In-memory computation
Parallel distributed processing
Can be used with many cluster managers (Spark, Yarn, Mesos e.t.c)
Polyglot(Spark provide high level API's to wrok with spark in Python , R, Java, Scala)
Cache & persistence

Advantage:
100X times faster than the original traditinal processing system.
10X times faster in secondary memory.

Architecture:
Driver program(Spark context)--Cluster Manager--worker nodes(Executor(cache, task))
When user submit its spark application code, driver converts actions and transformations ito logical DHE -> Physical execution plan with many stages ->Physical execution units(Tasks) -> Then these tasks are bundled and sent to cluster

Apache spark Eco-system
R, Python, Scala, java---Apache spark core API(Resposible for basic i/o function , scheduling and monitering)--Spark SQL, Spark streaming, Spark MlLib, Graphx

Cluster Manager Types
Standalone – a simple cluster manager included with Spark that makes it easy to set up a cluster.
Apache Mesos – Mesons is a Cluster manager that can also run Hadoop MapReduce and PySpark applications.
Hadoop YARN(Yet Another Resource Negotiator)  – the resource manager in Hadoop 2. This is mostly used, cluster manager.
Kubernetes – an open-source system for automating deployment, scaling, and management of containerized applications.
local – which is not really a cluster manager but still I wanted to mention as we use “local” for master() in order to run Spark on your laptop/computer.


Modules:
PySpark RDD (pyspark.RDD)
PySpark DataFrame and SQL (pyspark.sql)
PySpark Streaming (pyspark.streaming)
PySpark MLib (pyspark.ml, pyspark.mllib)
PySpark GraphFrames (GraphFrames)
PySpark Resource (pyspark.resource) It’s new in PySpark 3.0


Spark Web UI(http://localhost:4040/):
Apache Spark provides a suite of Web UIs (Jobs, Stages, Tasks, Storage, Environment, Executors, and SQL) to monitor the status of your Spark application.

Spark UI is separated into below tabs:-
	1. Spark Jobs
		Scheduling mode(Standalone , YARN, Mesos)
		Number of spark jobs(Spark jobs are equal to the number of actions in the application and each Spark job should have at least one Stage)
		Number of Stages
	2. Stages
		Note:
		1. The number of tasks you could see in each stage is the number of partitions that spark is going to work on and each task inside a stage is the same 		work that will be done by spark but on a different partition of data.
		2. Details of stage showcase Directed Acyclic Graph (DAG) of this stage, where vertices represent the RDDs or DataFrame and edges represent an operation to be applied.
	3. Tasks
	4. Storage
		The Storage tab displays the persisted RDDs and DataFrames, if any, in the application.
	5. Environment
		Runtime Information:	simply contains the runtime properties like versions of Java and Scala.
		Spark Properties: 	lists the application properties like ‘spark.app.name’ and ‘spark.driver.memory’.
		Hadoop Properties: 	displays properties relative to Hadoop and YARN. Note: Properties like ‘spark.hadoop’ are shown not in this part but in ‘Spark 						Properties’.
		System Properties: 	hows more details about the JVM.
		Classpath Entries: 	lists the classes loaded from different sources, which is very useful to resolve class conflicts.
	6. Executors
		The Executors tab displays summary information about the executors that were created for the application, including memory and disk usage and task and 		shuffle information. The Storage Memory column shows the amount of memory used and reserved for caching data.
	7. SQL
		If the application executes Spark SQL queries then the SQL tab displays information, such as the duration, Spark jobs, and physical and logical plans 		for the queries.


Spark History Server:Spark History servers, keep a log of all Spark application you submit by spark-submit, spark-shell.


Note:
1. Py4J is a Java library that is integrated within PySpark and allows python to dynamically interact with JVM objects, hence to run PySpark you also need 		Java to be installed along with Python, and Apache Spark.



7. Processing types:
	1. Shell mode(devlopment or adHoc queries)
	1. Batch 
	2. Real time(Streaming)



2. Pyspark SparkConf(SparkConf help us to setup confriguration to run a application locally or on cluster)
	Notes:
	
	
	
	Ex:
		from pyspark import SparkConf
		from pyspark.conf import SparkConf  
		conf= SparkConf().setAppName('PySpark Demo App').setMaster('local[2]') 
		#Here .setAppName(), .setMaster() are static method(Methods take same argument) and this conf object can be used in creation of SparkContext and 		 sparkSession object. 

	
2. Spark Context
	Notes:
	1. we can have multiple SparkSession object but only one SparkContext per JVM.
	Uses: Create RDD, Accumalator, Broadcast
	Ex:
		from pyspark import SparkContext
		sc=SparkContext.getOrCreate(conf)
		
		from pyspark.sql import SparkSession
		context= spark.sparkContext.getorCreate()
	
	Methods:
	1. accumulator:-It creates an accumulator variable of a given data type. Only a driver can access accumulator variables.
	2. applicationId – Returns a unique ID of a Spark application.
	3. appName – Return an app name that was given when creating SparkContext
	4. broadcast – read-only variable broadcast to the entire cluster. You can broadcast a variable to a Spark cluster only once.
	5. emptyRDD() – Creates an empty RDD
	6. getPersistentRDDs – Returns all persisted RDD’s
	7. getOrCreate – Creates or returns a SparkContext
	8. hadoopFile – Returns an RDD of a Hadoop file
	9. master()–  Returns master that set while creating SparkContext
	10. setLogLevel – Change log level to debug, info, warn, fatal and error
	11. textFile – Reads a text file from HDFS, local or any Hadoop supported file systems and returns an RDD
	12. union – Union two RDD’s
	13. wholeTextFiles – Reads a text file in the folder from HDFS, local or any Hadoop supported file systems and returns an RDD of Tuple2. First element of 	  the tuple consists file name and the second element consists context of the text file.



	

2. Spark session(Since Spark 2.0, SparkSession has become an entry point to Spark to programaticaly create RDD, DataFrame, and Dataset(Available in scala only) ):
	Uses:
		1. Dataframe, 
		2. For querying the dataframe(spark.sql())
	Note:
	1. SparkSession internally creates SparkConfig and SparkContext with the configuration provided with SparkSession(Here we can pass the SparkConf object to 		configure the properties).
	2.  You can create multiple SparkSession objects but only one SparkContext per JVM.
	3. In realtime application, you will pass master from spark-submit instead of hardcoding on Spark application.

	
	Ex: 	from pyspark.sql import SparkSession, SQLContext, HiveContext
		spark= SpakSession.builder.config(config).master("local[*]").AppName("Could be Anything").getorCreate()
	
	-----or------
		spark= SparkSession.newSession()
		
		spark.sparkContext.getConf().getAll()#Use to get all the configuration properties of spark or we can use get() method for specific property
		spark.sparkContext.getConf().get("spark.driver.host")
		


	
	Methods:
	1. spark.version#Returns Spark version where your application is running.
	2. builder –is used to create a new SparkSession, this return SparkSession.Builder
	3. createDataFrame() – This creates a DataFrame from a collection and an RDD
	4. emptyDataFrame() – Creates an empty DataFrame.
	5. getActiveSession() – returns an active Spark session.
	6. read() – Returns an instance of DataFrameReader class, this is used to read records from csv, parquet, avro and more file formats into DataFrame.
	7. readStream() – Returns an instance of DataStreamReader class, this is used to read streaming data. that can be used to read streaming data into 				DataFrame.
	8. sparkContext() – Returns a SparkContext.
	9. sqlContext() – Returns SQLContext.
	10. sql – Returns a DataFrame after executing the SQL mentioned.
	11. stop() – Stop the current SparkContext.
	12. udf() – Creates a Spark UDF to use it on DataFrame, Dataset and SQL.
		


		


2. RDD(Resilient(Able to recover quickly from difficult conditions) Distributed Dataset):-
	
	Note:
	1. RDD Lineage is also known as the RDD operator graph or RDD dependency graph.
	2. RDD is schema-less without column names and data type(structured and un-structured data).
	3. RDD cann have a name and it's unique identifier.

	We can create RDD from follwoing ways.
	1. Parallelize(sc.parallelize()):- Used when we want to create RDD from existing collection in your driver program.
		Ex:
		rdd=sc.parallelize([1,2,3,4,5])#returns a rdd from existing list having no of partition equal to the number of CPU cores.
		rdd=sc.parallelize([1,2,3,4,5], 5)#returns a rdd from existing list having no of partition equal to 5.
		
	2. From another RDD or dataframe(Using transformation operation on existing RDD)
		Ex:
		rdd= df.rdd #returns a rdd from dataframe
	3. From external system(Text file, HDFS etc)
		rdd=sc.textFile("c/*")	#Read all the text files within directory and create a single RDD. There should not be any sub-directory otherwise it will 					   through an error.
		rdd=sc.textFile("c/*",5)#RDD with 5 partitons
		rdd=sc.wholeTextFiles("c/*")	#Reads single or multiple files and returns a single RDD[Tuple2[String, String]], where first value (_1) in a tuple 						    is a file name and second value (_2) is content of the file.
		rdd=sc.wholeTextFiles("c/*",5)#RDD with 5 partitons
	

	Features of RDD:
	1. In memory computation
	2. Lazy evaluation
	3. Fault tolerant(PySpark task failures are automatically recovered for a certain number of times as per the configuration)
	4. Immutability
	5. Partioning(RDD is divided into logical sub parts): Can be changeable
	6. Persistance(RDD can be reused and can choose the storage stratgy)
	7. Coarsed gained operations(groupBy, Map etc): Any operation applied on RDD is coarse gained.



3. RDD Operations:	
		Normal:-
			1. Transaformation: Return the new RDD	
			
			Types:	1. Narrow(There will not be any data movement b/w partitions to execute narrow transformation) 
					Ex:  	 map(), mapPartition(), flatMap(), filter(), union()
				
				2. Wider(There will be data movements between partitions to execute wider transformations) or Shuffle transformation 
					Ex: 	groupByKey(), aggregateByKey(), aggregate(), join(), repartition(), coalesce(), reduceByKey(), cogroup() 
					Note:
					PySpark Shuffle is an expensive operation since it involves the following
					1. Disk I/O
					2. Involves data serialization and deserialization
					3. Network I/O
					
					
				
			Note: 
			1. When compared to Narrow transformations, wider transformations are expensive operations due to shuffling of data.
			2. When you dealing with less amount of data, you should typically reduce the shuffle partitions otherwise you will end up with 			many partitioned files with less number of records in each partition. which results in running many tasks with lesser data to 				process.
			3. when you have too much of data and having less number of partitions results in fewer longer running tasks and some times you 			may also get out of memory error.
			4. Getting the right size of the shuffle partition is always tricky and takes many runs with different values to achieve the 				optimized number. This is one of the key properties to look for when you have performance issues on PySpark jobs.
				
			
			Methods.
			cache(), filter(),flatmap(),map(), mapPartitions(), mapPartitionsWithIndex(), randomSplit(), union(), sample(), intersection(), 			distinct(), repartition(), coalesce()
			
			
			2. Action: return the values from an RDD to a driver program. 
					Ex: count(), collect(), first(), max(), reduce()
	
		Pair RDD Function(RDD with keyvalue pair):-
	
			1. Transaformation
			2. Action

			Ex:
			
3. Repartition and Coalesce Transformation: We can change the number of partition of RDD/Dataframe/Datasets at run time using 
rdd.repartition(numberOfPartition)/df.repartition(numberOfPartition)- To Increase/Decrease the number of partition. 
rdd.coalesce(numberOfPartition)/df.coalesce(numberOfPartition)- To Decrease the number of partition. 

Calling shuffle transformation on DataFrame results in shuffling data between multiple executors and even machines and finally repartitions data into 200 partitions by default. PySpark default defines shuffling partition to 200 using spark.sql.shuffle.partitions configuration.
spark.conf.set("spark.sql.shuffle.partitions", "500")
df = df.groupBy("id").count()
print(df4.rdd.getNumPartitions())#return 200 bydefault

Ex:	
rdd.getNumPartitions()#return the current number of partioin of an RDD.
df.rdd.getNumPartitions()#return the current number of partioin of an dataframe.


Note:
1. PySpark repartition() and coalesce() are very expensive operations as they shuffle the data across many partitions hence try to minimize using these as much as possible.



4. PySpark Cache and Persist are optimization techniques to improve the performance of the RDD jobs that are iterative and interactive.

   Notes:
   1. cache() method default saves it to memory (MEMORY_ONLY for rdd(meaning it will store the data in the JVM heap as unserialized objects), 		MEMORY_AND_DISK` for datafram or datasets because recomputing the in-memory columnar representation of the underlying table is expensive) whereas persist() method is used to store it to the user-defined storage level.
   
   2. StorageLevel(Here we can set the configuration on how should a RDD be stored. StorageLevel decides whether RDD should be stored in the memory or should it be  stored over the disk, or both. It also decides whether to serialize RDD and whether to replicate RDD partitions).
   
   3. When you persist a dataset, each node stores its partitioned data in memory and reuses them in other actions on that dataset. And Spark’s persisted data on nodes are fault-tolerant meaning if any partition of a Dataset is lost, it will automatically be recomputed using the original transformations that created it.
   
   4. Spark automatically monitors every persist() and cache() calls you make and it checks usage on each node and drops persisted data if not used or using least-recently-used (LRU) algorithm. we can also manually remove using rdd.unpersist() method.

   5. Spark caching and persistence is just one of the optimization techniques to improve the performance of Spark jobs.
   6. On Spark UI, the Storage tab shows where partitions exist in memory or disk across the cluster.
   7. Dataset cache() internally call persist(pyspark.StorageLevel.MEMORY_AND_DISK)
   8. Caching of Spark DataFrame or Dataset is a lazy operation, meaning a DataFrame will not be cached until you trigger an action. 


Storage Level    Space used  CPU time  In memory  On-disk  Serialized   Recompute some partitions
----------------------------------------------------------------------------------------------------
MEMORY_ONLY          High        Low       Y          N        N         Y    
MEMORY_ONLY_SER      Low         High      Y          N        Y         Y
MEMORY_AND_DISK      High        Medium    Some       Some     Some      N
MEMORY_AND_DISK_SER  Low         High      Some       Some     Y         N
DISK_ONLY            Low         High      N          Y        Y         N
MEMORY_ONLY_2  	     Low         High      Some       Some     Y         N
MEMORY_AND_DISK_2    Low         High      N          Y        Y         N


Ex:
	import pyspark
	rddcache = rdd.cache()
	rddPersist = rdd.persist() # Here we have two diff signature for persist method. by-defult it will be MEMORY_ONLY
	rddPersist = rdd.persist(pyspark.StorageLevel.MEMORY_ONLY)
	


5. Dataframe(DataFrame is a distributed collection of data organized into named columns(With inbulit richer optimizations)

	Note:
	1. PySpark DataFrames are distributed in the cluster (meaning the data in DataFrame’s are stored in different machines in a cluster) and any operations in PySpark executes in parallel on all machines.
	2. Datframe can be created from follwoing ways.
		RDD, Python list, External sources(DataFrame has a rich set of API which supports reading and writing several file formats)
		
	Ex:
	df= rdd.toDF() #return a dataframe with columns name as _1 or _2.
	df= rdd.toDF('col1','col2') #For a RDD(list of tuple of two elements)return a dataframe with columns name as col1 or col2.
		Note: 
		1. By default, the datatype of these columns infers to the type of data. We can change this behavior by supplying schema, where we can 			specify a column name, data type, and nullable for each field/column.
	df =spark.dataframe(rdd).toDF("col1","col2")#### Needs to check
	
	3. Process structured, semi-structured  and unstructured data.
	4. 

	
	Example:
		df.show() // Show 20 rows & 20 characters for columns
		df.show(50) // Show 50 rows
		df.show(false) // Show 20 rows with full column value
		df.show(50,false) // Show 50 rows & full column value
		df.show(20,20,true) // Show 20 rows, column length 20 & displays data in vertical
		df.printschema() #it will print the schema
		Output:	root
				|-- _1: string (nullable = true)
 				|-- _2: string (nullable = true)
			
	
	Features:
	1. Immutability
	2. Lazy Evaluation
	3. Distributed storage
	4. Fault tolrance
	5. Inbuild-optimization when using DataFrames


6. PySpark SQL:
	Pre-Requesties:Need a dataframe on which we have to query the data.

	Hive v/s Spark SQl
	Hive uses map reduce which lags in performance with small & medium sized datasets.
	No resume capability.
	Hive can't drop encrypted database. 


	Apache spark SQL advantage over hive.
	1. Faster
	2. No migration hurdle(we can use metastore service of pre-written hive quesries to run it using spark sql, No need to transfering hive queries to spark SQL )	
	3. Real time querying

	Features:
	1. Supports ANSI SQL
	2. support structure & semi-sturctured data
	3. support varioud data fromats
	4. SQl queries can be converted into RDDs for transformation.
	5. Standard JDBC/ODBC connectivity
	6. Let's you define user defined function 

	API's
	1. Datasource API
	2. DataFrame API
	3. Interpretation and Optimization(It handle the functional programming part of SQL)
	4. SQl Service API

	 
	Note:
	1. In order to use RAW SQL. Create a temporary view on DataFrame and Once it is created it will be accessible throughout the spark session and will get 		destory with SparkContext.
	
Example:
	df.createOrReplaceTempView("PERSON_DATA")#Here df is datframe on which we want to use ANSI SQL and we have created a PERSON_DATA named temp view that we 						   will be using in our query.
	df2 = spark.sql("SELECT * from PERSON_DATA")#It will create another dataframe as df2.
	df2.printSchema()
	df2.show()	
	

7. Spark streaming(Technique of transfering data so that it can be processed as a staedy and contunous stream ): 

	Features:
	1. Scaling
	2. Speed(low latency)
	3. Fault tolrent
	4. Integration(with real time and batch time processing)
	5. Busines analytics(Used to track behaviour customer)

Note:
The fundamental stream unit is Dstream(Discretized stream) which is basically a series of of RDD's to process the real-time data.
			

7. PySpark GraphFrames(PySpark GraphFrames are introduced in Spark 3.0 version to support Graphs on DataFrame’s. Prior to 3.0, Spark has GraphX library which ideally runs on RDD and loses all Data Frame capabilities.)
	Note:
	1. GraphFrames is a package for Apache Spark which provides DataFrame-based Graphs. It provides high-level APIs in Scala, Java, and Python. It aims to provide both the functionality of GraphX and extended functionality taking advantage of Spark DataFrames. This extended functionality includes motif finding, DataFrame-based serialization, and highly expressive graph queries.
	2. GraphX works on RDDs where as GraphFrames works with DataFrames.
	




5. Shared variables in Spark	


1. Broadcast(read-only shared variable) (Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, spark distributes broadcast variables to the machine using efficient broadcast algorithms to reduce communication costs)

	Use case
		
	1.Assume you are getting a two-letter country state code in a file and you wanted to transform it to full state name, (for example CA to California, NY to 	     New York e.t.c) by doing a lookup to reference mapping. In some instances, this data could be large and you may have many such lookups (like zip code).
	  Instead of distributing this information along with each task over the network (overhead and time consuming), we can use the broadcast variable to cache 	     this lookup info on each machine and tasks use this cached info while executing the transformations.	

				The broadcasted data is cache in serialized format and deserialized before executing each task.			
	
				broadcastVar = sc.broadcast([0, 1, 2, 3])
				broadcastVar.value
				
	Note:
	1. Broadcast variables are not sent to executors with sc.broadcast(variable) call instead, they will be sent to executors when they are first 		used(broadcastVar.value)
	
				
2. Accumaltor(Updatable shared variables): Accumulator variables are used for aggregating the information through associative and commutative operations.
	
	Types:
		1. Named accumulators(Can be visible in spark web-UI Accumaltor tab, On this tab we can see two tables) 
						-First table “accumulable” – consists of all named accumulator variables and their values. 
						-Second table “Tasks” – value for each accumulator modified by a task.
		2. Unnamed accumulators(Can't be visible in spark web-UI)

	
	Notes:
	1. PySpark by default supports creating an accumulator of any numeric type and provides the capability to add custom accumulator types. 
	2. PySpark by default provides accumulator methods for long, double and collection types.
	

		


PySpark Accumulators are shared variables that can be updated by executors and propagates back to driver program. These variables are used to add sum or counts and final results can be accessed only by driver program.		

	Ex:
		accum = sc.longAccumulator("SumAccumulator")
		sc.parallelize([1, 2, 3]).foreach(lambda x: accum.add(x))


	
7. Pyspark SparkFiles(Upload file using sc.addFile() and get the path on a worker using SparkFiles.get()) 
	Ex:
		SparkContext.addFile().


9. PySpark - MLlib


10. PySpark - Serializers(Serialization is used for performance tuning on Apache Spark,All data that is sent over the network or written to the disk or persisted in the memory should be serialized.)

	MarshalSerializer
	Serializes objects using Python’s Marshal Serializer. This serializer is faster than PickleSerializer, but supports fewer datatypes.
	
	PickleSerializer
	Serializes objects using Python’s Pickle Serializer. This serializer supports nearly any Python object, but may not be as fast as more specialized 		serializers.

Drawbacks of Hive
It cannot resume processing, which means if the execution fails in the middle of a workflow, you cannot resume from where it got stuck.
We cannot drop the encrypted databases in cascade when the trash is enabled. It leads to the execution error. For dropping such type of database, users have to use the Purge option.
The ad-hoc queries are executed using MapReduce, which is launched by the Hive but when we analyze the medium size database, it delays the performance.
Hive doesn't support the update or delete operation.
It is limited to the subquery support.



Question:
1. What is spark?- Spark is open source ,Scalable , Parllel ,In-memory enviornment of analytics engine.
2. Why dataset is faster than the dataframe ?- b/e dataset has implemented a encoder mechanism
3. Why we need RDD or Pyspark RDD limitation?
4. Read a CSV file with two column as RDD and check is it list of tuple of two elements?
5. Read a text file with two column as RDD and check is it list of tuple of two elements?

